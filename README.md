# AI_Introduction
1. 인공지능에서 지능에 해당하는 기능은 무엇인가?
    - 
2. 인공지능의 종류 3가지에 대해서 설명하시오 (지도학습, 반지도학습, 강화학습)
   -
3. 전통적인 프로그래밍 방법과 인공지능 프로그램의 차이점은 무엇인가?
   -
4. 딥러닝과 머신러닝의 차이점은 무엇인가?
   -
5. Classification과 Regression의 주된 차이점은?
   -
6. 머신러닝에서 차원의 저주(curse of dimensionality)란?
   -
7. Dimensionality Reduction는 왜 필요한가?
   - 
## 8. Ridge와 Lasso의 공통점과 차이점? (Regularization, 규제 , Scaling)
### 라쏘와 릿지의 등장배경
- 다중 선형 회귀모델은 과적합 되는 경향이 존재했음
- 필요 이상으로 자세하게 특징값과 라벨값의 고계를 분석했다는 것이다.
- 이로 인해 일반화 능력(범용 능력)이 떨어져 새로운 데이터를 제대로 예측하지 못했음
- 이를 해결하기 위해 나온것이 라쏘와 릿지이다. 
### 라쏘
- 기존의 선형 회귀에선 적절한 가중치와 편향을 찾아내는 것이 중요했음
- 라쏘는 거기에 추가적인 제약 조건을 준다.
- 바로 MSE가 초쇠가 되게하는 가중치와 편향을 찾으면서 동시에, 가중치들의 절대값의 합이 최소가 되게 한다.
- 즉, 가중치의 모든 원소가 0이 되거나 0에 가깝게 되도록 해야 한다.
- 따라서 어떠한 특징들을 모델을 만들 때 사용되지 않기도 함
- 어떤 백터 요소의 절댓값의 합은 L1-norm이다.
- 즉, 라쏘는 L1-norm 패널티를 가진 선형 회귀 방법임
- 또한 MSE와 penalty항의 합이 최소가 되게 하는 w와b를 찾는것이 라쏘의 목적이다.
### 릿지
- 패널티 항에 L1-norm대신 L2-norm 패널티를 가진다.
- 릿지의 가중치들은 0에 가까워질 뿐 0이 되지는 않는다.
### 공통점
1.정규화
- 두 모델 모두 회귀 계수의 크기를 제한하여 과적합을 방지한다.
- 손실 함수와 패널티를 추가하여 모델이 너무 복잡해지는것을 막는다.

2. 스케일링이 중요
- 릿지와 라쏘는 특성들의 크기에 영향을 받으므로, 표준화를 필수적으로 적용해야 한다.
- 일반적으로 평균 0, 분산 1로 정규화한 후 사용하는 것이 효과적이다.

### 차이점
|      | 릿지 | 라쏘 |
|------|---| ---------- |
| 규제방식 | L2정규화 적용 | L1정규화 적용 |
|특징선택| 모든 특성이 포함되지만, 계수의 크기를 줄임| 불필요한 특성의 계수를 0으로 만들어 자동으로 특성 선택 수행|
|계수 감소 방식| 가중치(회귀 계수)가 0에 가깝게 축소되지만 완전히 0이 되지는 않음\ 특정 가중치가 완전히 0이 되어 모델이 희소 해짐|
|사용 예시| 다중 공선성 문제 해결에 효과적| 중요하지 안흥ㄴ 특성을 제거하여 해석 가능한 모델을 만들 때 유용|
|성능 차이| 모든 특성을 사용하므로 일반적으로 예측 성능이 안정적 | 차원이 높은 데이터에서 변수선택이 필요할 때 더 효과적|
    

### 결론
- 특성이 많은데 그중 일부분만 중요하다면 라쏘
- 특성의 중요도가 전체적으로 비슷하다면 릿지가 적합하다.
### 참고
[선형회귀, 라쏘, 릿지](https://otugi.tistory.com/127)

## 9. Overfitting(과적합) vs. Underfitting(과소적합)
|   | 과적합                        |과소적합|
|----|----------------------------|-----|
|정의| 훈련 데이터에 너무 잘맞아서 테스트 데이터에서 성능이 저하됨 | 훈련 데이터 조차 제대로 학습하지 못해 성능이 낮음|
|원인| 모델이 너무 복잡함                 | 모델이 너무 단순함|
|증상| 훈련데이터 성능 높고, 테스트 데이터 성능 낮음 | 훈련 데이터 성능 낮고, 테스트 데이터 성능 낮음|
|해결책| 모델 단순화, 정규화(릿지,라쏘), Dropout 사용, 데이터 증가| 모델 복잡화, 학습 시간 증가, 더 많은 특성 추가|
|예시| 다항 회귀에서 너무 높은 차수를 사용할 때| 선형 회귀로 비선형 데이터를 학습 할 때|

10. Feature Engineering과 Feature Selection의 차이점은?
### Feature Engineering(특성 공학)
#### 정의
- 새로운 특성을 생성하거나 변형하여 모델의 성능을 높이는 과정
- 원본 데이터에서 유용한 정보를 추출하여 모델이 학습하기 쉬웅ㄴ 형태로 변환하는 것이 목적

#### 주요 기법
- 특성 변환(Transformation)
  - 예: 로그변환, 제곱근 변환, 표준화
- 특성 조합(Feature Combination)
  - 여러 개의 특성을 조합하여 새로운 특성을 생성
  - 예: 키 x 몸무게 -> BMI(체질량 지수)
- 다항식 특성 추가
  - 비선형 관계를 포착하기 위해 새로운 다항식 특성을 추가
    - 예: x -> x^2, x^3
- 텍스트 데이터 처리
  - 단어 임베딩, TF-IDF 변환 등
- 시간 관련 특성 추가
  - "날짜" 데이터를 활용해 연도, 월, 요일, 계절 정보 추가

#### 목표
- 원본 데이터가 가진 정보를 최대한 활용하여 모델 성능을 극대화
- 머신러닝 모델이 더 좋은 패턴을 학습할 수 있도록 새로운 특성을 추가


### Feature Selection(특성 선택)
#### 정의
- 불필요한 특성을 제거하여 모델의 성능을 최적화 하는 과정
- 모든 특성이 유용한 것은 아니므로, 모델 성능에 기여하지 않는 특성을 제거하여 차원을 줄이고 학습 속도를 높임

#### 주요기법
- 필터 방식
  - 특성과 타겟 변수 간의 상관관계를 분석하여 중요한 특성만 선택
  - 예: 피어슨 상관계수, 분산 임계값
- 랩퍼 방식
  - 모델을 사용하여 반복적으로 특성을 추가/제거하며 최적의 조합을 찾음
  - 예: RFE
- 임베디드 방식
  - 모델이 학습하면서 자동으로 중요한 특성을 선택
  - 예: 라쏘 회귀, 랜덤 포레스트의 feature Inportanace

#### 목표
- 모델이 불필요한 특성으로 인한 과적합을 방지
- 모델의 계산 비용을 줄이고, 해석 가능성을 높임


## 11. 전처리(Preprocessing)의 목적과 방법? (노이즈, 이상치, 결측치)
### 정의
- 전처리는 모델이 데이터를 효과적으로 학습할 수 있도록 데이터를 정리하고 변환하는 과정

### 목적
#### 데이터 품질 향상
- 데이터에 포함된 오류, 결측값, 이상치, 중복 데이터 등을 처리하여 신뢰성 있는 데이터를 제공
- 정제된 데이터는 모델의 예측 성능을 향상시킴

#### 모델 성능 최적화
- 머신러닝 모델은 데이터의 스케일, 분포, 특성 등에 민감
- 올바르게 전처리된 데이터는 모델이 학습하는데 적합하며, 과적합 또는 학습 오류를 줄일 수 있음

#### 계산 효율성 증가
- 데이터가 클수록 학습 시간이 증가하므로, 불필요한 데이터 제거 및 변환을 통해 속도를 개선
- 예: 차원축소 등을 활용하여 모델의 연산량을 줄일 수 있음

### 전처리 방법
#### 노이즈 제거
- 노이즈란 데이터 내에서 원래 정보가 아닌 불필요한 값, 오류, 무작위 변동성을 의미
- 예: 잘못된 센서 데이터, OCR 인신 오류, 텍스트 데이터의 오타 등
- 해결방법으로, 평균필터, 로우패스 필터, 텍스트 정제, 이미지 노이즈 제거가 있다.

#### 이상치 처리
- 이상치란 일반적인 데이터 분포에서 벗어난 비정상적인 값을 의미하낟.
- 예: 신용카드 사기 거래 탐지, 센서 오작동 값, 데이터 입력 오류
- 이상치 탐지방법에는 IQR, Z-Score- Isolation Forest, DBSCAN이 있다.

#### 결측치 처리
- 결측치란 데이터에서 값이 존재하지 않는 경우를 말한다.
- 설문조사에서 응답 누락, 센서 데이터 손실, 잘못된 데이터 수집
- 해결방법: 삭제, 평균/중앙값 대체, 최빈값 대체, KNN Imputation이 있다.

### 결론
- 전처리는 머신러닝에서 가장 중요한 과정 중 하나이며, 데이터를 정리하고 모델의 학습을 최적화 하는데 필수적이다.
- 노이즈제거: 데이터에서 불필요한 정보를 제거
- 이상치 처리: 극단적인 값이 모델 성능을 해치지 않도록 조정
- 결측치 처리: 데이터 누락 문제를 해결하여 신뢰성 향상

12. EDA(Explorary Data Analysis)란? 데이터의 특성 파악(분포, 상관관계)

### 정의
- 데이터 분선의 첫 단계로, 데이터의 구조와 특성을 파악하고, 이상치나 결측치를 발견하며, 데이터의 분포와 상관관계를 시각적으로 탐색하는 과정이다.
- EDA를 수행하면 데이터의 패턴을 이해하고, 머신러닝 모델을 효과적으로 설계하기 위한 인사이트를 얻을 수 있다.

### 목적
- 데이터의 기본 구조 파악: 데이터의 크기, 타입, 기초 통계를 확인
- 결측치 및 이상치 탐색: 데이터의 누락된 값이나 이상값을 찾음
- 데이터 분포 분석: 변수의 분포를 확인하여 정규성 여부 등을 판단
- 변수 간의 관계 분석: 상관관계, 다변수 관계 분석

### EDA 과정
1. 데이터 기본 정보확인
- 먼저 데이터의 크기(행, 열 개수)와 변수(칼럼)의 타입을 확인한다.

2. 기초 통계확인
- 각 컬럼별로 데이터의 평균, 중앙값, 표준편차, 최솟값, 최댓값을 확인한다.

3. 결측치 및 이상치 탐색
- 결측치와 이상치를 탐색한 뒤 처리한다. 

4. 데이터 분포 시각화
- 히스토그램을 통한 분포를 확인
- 정규분포인지 확인하여 데이터 스케일링 방법 결정

5. 변수 간의 관계 분석
- 수치형 변수 간의 관계를 확인할 때 상관계수를 분석한다.

### 결론
- EDA를 통해 얻은 인사이트를 바탕으로 데이터 전처리 및 모델 설계 전략을 수릭할 수 있다.
- EDA는 모델을 설계하기 전, 데이터를 이해하고 적절한 전처리 방안을 결정하는 중요한 과정이다.


13. 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관되는가?
### 개념
- 회귀분석에서 절편, 기울기가 데이터를 설명하는 중요한 요소임
- 이는 선형 회귀 뿐아니라, 딥러닝에서도 가중치와 편향 개념으로 확장된다.

### 의미
- 회귀 모델은 일반적으로 1차 함수인 선형 회귀로 표현됨
- 즉, y = ax + b
  - a: 기울기
  - b: 절편
  - x: 독립 변수
  - y: 종속 변수
- 즉, 기울기와 절편을 이용하여 입력값 x에 대해 y값을 예측하는 것이 회귀 분석의 핵심이다.
- 여기서 기울기는 데이터의 변화율을 설명하고, 절편은 기준값을 설정하는 역할을 한다.

### 딟러닝과 회귀 분석
- 딥러닝에서 회귀분석의 기울기와 절편 개념은 가중치와 편향으로 확장된다.

#### 가중치(weight)
- 선형 회귀에서 기울기 a에 해당한다.
- 뉴런이 입력 데이터를 처리하는 중요도를 나타내는 계수
- 모델이 학습하면서 최적의 가중치를 찾아 데이터의 패턴을 학습
- 예시: y = w[0]x[0] + w[1]x[1] + w[2]x[2] + b
- w1,w12,w3는 각 입력 변수에 곱해지는 가중치
- b는 편향 즉, 절편을 의미
- 즉, 뉴런은 여러 개의 입력을 받고, 각 입력에 가중치를 곱한 후 합산하여 출력값을 결정

#### 편향(Bias)
- 절편에 해당
- 뉴런이 입력이 0이어도 일정한 출력을 내게 하는 역할
- 딥러닝 모델이 특정 패턴을 학습할 수 있도록 유연성을 제공
- 예: y= w[0]x[0] +b
- x[0] = 0이면, y = b
- 즉, 편향은 모델이 입력이 없어도 학습된 기본값을 유지하도록 도와줌

#### 선형 회귀와의 유사성
|개념| 선형회귀 | 딥러닝(뉴런)|
|-|-|-|
|기울기| 독립 변수 x의 변화량| 가중치(W)|
|절편| x = 0일때의 기본값 | 편향(Bias,b)|
|예측값| y=ax+b| y=Wx+b|
|학습과정| 최소제곱법으로 최적의 기울기, 절편 찾음| 경사하강법으로 최적의 가중치, 편향 찾음|

#### 경사 하강법 학습 과정
1. 초기 가중치와 편향을 랜덤하게 설정
2. 손실 함수를 계산
3. 기울기를 계산하여 가중치 업데이트
   -편미분을 이용해 각 가중치 W와 편향 b에 대한 변화량 계산
4. 반복적으로 업데이트하여 최적의 값 찾기

#### 결론
- 회귀 분석의 기울기와 절편은 딥러닝에서 가중치와 편향으로 확장
- 기울기는 입력 변수의 변화량을 설명
- 절편은 기본값을 설정
- 딥러닝에서는 경사 하강법을 통해 최적의 가중치와 편향을 학습하여 데이터의 패턴을 학습함
- 선형 회귀는 딥러닝의 기본 개념으로, 신경망에서도 가중치와 편향을 학습하는 방식이 유사함
- 즉, 회귀 모델은 딥러닝의 가장 기초적인 형태이며, 신경망이 더 복잡해지면 다층 퍼셉트론(MLP), CNN, RNN 등으로 확장될 뿐, 기본 원리는 동일

##14. GRU을 사용하는 이유와 차별성은?
### 정의
- GRU(gated Recurrent Unit)는 RNN의 한 종류이며, LSTM과 유사하지만 더 간단한 구조를 갖는다.
- RNN의 문제점을 개선하면서 시계열 데이터, 자연어 처리(NLP), 음성인식 등 다양한 분야에서 사용됨

### 사용 이유
#### 기존 RNN의 한계 
- 장기 의존성 문제
  - 일반적인 RNN은 이전 정보를 현재 상태로 전달하지만, 오래된 정보를 잊어버리는 경향이 있음
  - 즉, 긴 문장이나 긴 시계열 데이터에서는 초반 정보가 손실된다.
- 기울기 소실 문제
  - RNN은 역전파 과정에서 기울기가 점점 작아지는 문제가 발생
  - 이로 인해 초반 데이터가 학습되지 않고, 가중치 업데이트가 잘 되지 않음

#### GRU의 장점
- LSTM(Long Short-Term Memory)처럼 게이트 구조를 사용하여 정보 흐름을 조절하여 장기 의존성을 해결
- LSTM보다 구조가 단순하여 계산 속도가 빠름
- 적은 데이터로도 효과적인 성능을 낼 수 있음

### GRU의 차별점
- GRU는 LSTM의 Cell State를 제거하고, Hidden State 하나로만 정보 전달
- GRU는 Reset Gate와 Update Gate만 사용하여 LSTM보다 구조가 단순함
- GRU는 연산량이 적어 빠르지만, LSTM은 더 정밀한 패턴 학습이 가능함

## 15. 결정트리에서 불순도(Impurity) – 지니 계수(Gini Index)란 무엇인가?
### 불순도
#### 정의
- 불순도는 결정 트리에서 특정 노드의 데이터가 얼마나 혼합 되어있는지를 나타내는 지표이다.
- 즉, 한 노드에 다양한 클래스가 섞여 있을수록 불순도가 높으며, 한 가지 클래스만 포함되면 불순도가 낮음
- 결정트리는 데이터를 가장 잘 구분하는 기준을 찾아 트리를 분할한다.
- 트리가 분할될 떄, 불순도가 낮아지는 방향으로 최적의 기준을 찾음
- 불순도가 낮아질수록 예측이 정확해지고, 모델의 성능이 향상됨

#### 측정방법
- 불순도는 지니계수와 엔트로피 방식으로 측정이 가능하다.
- 지니계수: 각 클래스로 데이터가 분포될 확률을 사용하여 불순도를 측정
- 엔트로피: 정보이론 기반의 개념을 활용하여 불순도를 측정

### 지니 계수
#### 정의
- 지니 계수는 결정트리에서 특정 노드가 얼마나 순수(Pure)한지 측정하는 지표
- 지니 계수는 클래스가 섞여 있을 확률을 기반으로 계산된다.
- 즉, 모든 클래스의 확률을 제곱한 값을 합한한 후, 1에서 뺴는 방식으로 계산 된다.
- 불순도가 높을수록 지니계수가 커지고, 불순도가 낮을수록 지니 계수가 작아진다.

### 결론
- 지니계수는 결정트리에서 불순도를 측정하는 중요한 지표
- 클래스가 얼마나 혼합되어 있는지 수치적으로 나타내며, 값이 낮을 수록 데이터가 순수
- 결정트리는 지니계수가 가장 작은 방향으로 데이터를 분할하여 예측 성능을 향상
- 지니 계수는 엔트로피보다 계산량이 적어 빠르고, 대부분의 결정트리 모델에서 기본값을 사용됨
- 즉, 지니계수는 '가장 순수한 분할을 찾기 위한 기준'이다.


## 16. 앙상블이란 무엇인가?
### 정의
- 앙상블이란 여러 개의 머신러닝 모델을 조합하여 최종적인 예측 성능을 향상시키는 기법이다.
- 즉, 개별 모델의 단점을 보완하고 강점을 극대화하여 더 정확하고 안정적인 결과를 얻는 방법이다.

### 사용이유
#### 단일 모델의 한계
- 하나의 모델이 데이터를 학습하면, 특정한 패턴에 과적합 될 가능성이 높음
- 단일 모델은 특정 유형의 데이터에는 강하지만, 다른 유형에서는 약할 수 있음
- 즉, 여러 개의 모델을 결합하여 개별 모델보다 더 강력한 성능을 보일 수 있다.

#### 장점
- 정확도 향상: 여러 모델의 결과를 종합하면 일반적으로 성능이 향상됨
- 과적합 방지: 개별 모델이 과적합되더라도, 여러 모델을 조합하면 일반화 성능이 높아짐
- 안정성 증가: 한 개의 모델이 잘못된 예측을 하더라도 다른 모델들이 이를 보완


## 17. 부트 스트랩핑(bootstraping)이란 무엇인가?
### 정의
- 주어진 데이터에서 여러 개의 샘플을 무작위로 추출하여 새로운 데이터셋을 만드는 통계적 기법
- 이를 사용하면 데이터가 부족한 상황에서도 다양한 샘플을 만들어 신뢰도 높은 모델을 만들 수 있음

### 방식
1. 원본 데이터셋에서 중복을 허용하여 랜덤하게 샘플을 추출한다.
2. 동일한 크기의 새로운 데이터셋을 여러개 생성
3. 각 샘플을 독립적으로 학습하여 결과를 종합
4. 예시
   ```
       원본 데이터: [A, B, C, D, E]
       부트스트랩 샘플 1: [B, C, C, E, A]
       부트스트랩 샘플 2: [E, D, D, A, C]
       부트스트랩 샘플 3: [A, A, B, D, E]
    ```
### 사용 이유
#### 데이터가 부족할 떄 효과적인 학습 방법
- 기존 데이터에서 랜덤 샘플링을 통해 여러 개의 학습용 데이터 셋을 생성
- 작은 데이터셋에서도 통계적으로 신뢰할 수 있는 결과 도출 가능

#### 추정치의 신뢰도 향상
- 단일 샘플이 아닌 여러 개의 부트스트랩 샘플을 활용하여 평균, 분산 등을 계산
- 모델의 안정성을 높이고, 과적합을 방지할 수 있음 

#### 앙상블 학습의 핵심
- 배깅에서 부트스트래핑을 사용하여 랜덤한 데이터셋을 생성

## 18. 배깅(Bagging)이란 무엇인가?
### 정의
- 여러 개의 약한 학습기(모델)를 독립적으로 학습한 후, 예측 결과를 평균(회귀) 도는 투표(분류)하여
- 최종 결과를 결정하는 앙상블 기법
- 배깅은 부트스트래핑을 활용하여 데이터 샘플을 랜덤하게 생성하고, 여러 개의 모델을 학습하여 예측을 안정화 하는 것이다.

### 배깅 과정
1. 부트스트램 샘플링
- 원본 데이터에서 중복을 허용하여 여러 개의 새로운 데이터셋을 생성
- 예를 들어, 원본 데이터가 100개라면, 100개를 랜덤하게 뽑되 중복이 가능

2. 독립적인 모델 학습
- 여러 개의 모델을 각각 다른 부트스트랩 샘플에 대해 학습

3. 예측값을 결합하여 최종 예측 수행
- 분류: 다수결 투표
- 회귀: 평균

### 사용이유
#### 단일 모델의 한계를 극복
- 하나의 모델이 과적합될 가능성이 높음
- 여러개의 모델을 조합하면 과적합을 방지
- 데이터의 작은 변화에 따라 결과가 크게 달라지는 문제 해결 가능

#### 과적합 방지
- 랜덤한 부트스트랩 샘플을 사용하여 모델을 독립적으로 학습하면, 개별 모델이 특정 패턴에 과적합 되는 것을 방지

#### 일반화 성능 향상
- 배깅을 사용하면 편향은 그대로 유지되지만, 분산이 줄ㄹ어들어 더 일반화된 모델을 생성

### 결론
- 배깅은 데이터의 랜덤성을 활용하여 안정적인 예측을 수행하는 강력한 방법이다. 

## 19. 주성분 분석(PCA) 이란 무엇인가?
### 정의
- 고차원의 데이터를 저차원으로 변환하여 핵심 정보를 유지하면서 **차원을 축소**하는 방법이다. 
- 즉, 데이터의 중요한 패턴을 유지하면서 불필요한 특성을 제거하여 더 효율적으로 분석할 수 있도록 변환하는 기법

### 필요한 이유
1. 고차원 데이터의 시각화
- 차원이 높은 데이터를 2D 또는 3D로 변환하여 시각화 가능

2. 차원의 저주 해결
- 차원이 높아질수록 모델의 성능이 저하될 수 있음

3. 연산 속도 향상
- 머신러닝 모델 학습 시, 차원이 낮으면 연산량이 줄어 속도가 빨라진다.

4. 중복 정보 제거
- 서로 상관관계가 높은 특성을 줄여 데이터의 핵심 정보를 유지

### 작동 원리
1. 데이터를 평균이 0이 되도록 정규화
2. 공분산 행렬 계산
3. 공분산 행렬의 고유값과 고유벡터 계산
4. 가장 큰 고유값을 가진 고유벡터를 주성분으로 선택
5. 데이터를 주성분 방향으로 변환하여 새로운 저차원 데이터 생성
즉, 데이터를 가장 잘 설명하는 새로운 축을 찾고 중요한 축을 몇개만 선택하여 차원을 축소하는 방법