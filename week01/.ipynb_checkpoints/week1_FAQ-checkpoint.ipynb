{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. 인공지능에서 지능에 해당하는 기능은 무엇인가?\n",
    "- 학습: 경험으로부터 데이터를 분석하고 패턴을 발견하는 능력\n",
    "- 추론: 주어진 정보를 바탕으로 새로운 결론을 도출하는 능력\n",
    "- 문제해결: 특정 목표를 달성하기 위한 최적의 방법을 찾는 능력\n",
    "- 지각: 외부 환경을 인식하고, 센서를 통해 들어온 정보를 구조화, 해석하는 능력\n",
    "- 자연어 처리: 인간의 언어를 이해하고 생성하는 능력\n"
   ],
   "id": "26eea6f10409f8cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 인공지능의 종류 3가지에 대해서 설명하시오 (지도학습, 반지도학습, 강화학습)\n",
    "### 지도학습\n",
    "- 입력 데이터(특징)와 그에 대응하는 정답(label)이 주어진 상태에서 모델을 학습 시키는 방법\n",
    "- 즉, 라벨이 있는 데이터를 통해 직접적으로 정답을 예측하는 모델을 학습\n",
    "- 장점: 명확한 정답이 있어 학습이 직관적이며 모델 성능을 평가하기 쉬움\n",
    "- 단점: (라벨링 작업을 위한)인적,비용적 자원이 많이 필요\n",
    "\n",
    "### 반지도학습\n",
    "- 소량의 라벨이 있는 데이터와 대량의 라벨이 없는 데이터를 함께 사용하여 모델을 학습\n",
    "- 즉, 소량의 라벨과 대량의 비라벨 데이터를 함께 사용하여 효율적으로 학습\n",
    "- 장점: 라벨이 적어도 비라벨 데이터를 잘 활용만 하면 모델 성능을 크게 향상시킬 수 있음\n",
    "- 단점: 비라벨 데이터를 어떻게 활용할지가 핵심 연구 주제\n",
    "  - 여기에는 클러스터링, 자기지도 학습 등 기법이 필요\n",
    "  ```\n",
    "    클러스터링: 데이터의 유사성을 기준으로 자연스러운 그룹을 형성하여 데이터의 구조를 파악하는 기법\n",
    "    자기지도 학습: 데이터 자체에서 레이블을 만들어내어 모델이 스스로 유용한 표현을 학습하도록 하는 방법\n",
    "    ```\n",
    "\n",
    "### 강화학습\n",
    "- 에이전트가 환경과 상호작용하면서 보상을 받고, 보상을 최대화하는 방향으로 정책을 학습하는 방법\n",
    "  - 여기서 에이전트는 환경과 상호작용하면서 스스로 학습하는 프로그램이나 알고리즘을 의미\n",
    "  - 즉 자동화된 시스템을 말한다.\n",
    "  - 예시로, 로봇제어에서는 로봇이 에이전트가 된다.\n",
    "  - 또한, 사용자 추천 시스템은 알고리즘이 에이전트가 됨\n",
    "- 즉, 환경과의 상호작용을 통해서 보상을 최대화하는 행동 전략을 학습\n",
    "- 학습구조\n",
    "  1. 에이전트가 환경에 행동을 취한다.\n",
    "  2. 환경은 에이전트에게 상태와 보상을 반환\n",
    "  3. 에이전트는 보상을 최대화하기 위해 행동 방식을 개선한다.\n",
    "- 장점: 명시적인 라벨 없이도 보상 신호를 통해서 학습이 가능하므로, 복잡한 환경에서 자율적 학습이 가능\n",
    "- 단점: 충분한 보상을 얻기까지 시뮬레이션 또는 실제 환경에서 많은 시도와 시행착오가 필요하며, 환경 설계가 중요하다.\n"
   ],
   "id": "c525bf80bdd2651b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. 전통적인 프로그래밍 방법과 인공지능 프로그램의 차이점은 무엇인가?\n",
    "### 전통적인 프로그래밍 방법\n",
    "- Data를 주면 프로그래머가 규칙을 만들어 결과를 출력하는 과정이였음\n",
    "- 즉, Data -> Rule -> Result 방식이였다.\n",
    "\n",
    "### 인공지능 프로그램\n",
    "- Data와 결과를 인공지능에게 주면 규칙을 만들어준다.\n",
    "- 즉, Data, Result -> Rule\n",
    "\n",
    "### 정리\n",
    "- 전통: input을 넣고 규칙을 만들면 ouput이 나오는 방식\n",
    "- 인공지능: input, ouput을 넣으면 규칙을 만들어주는 방식\n"
   ],
   "id": "3b0051b4e1b7575e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. 딥러닝과 머신러닝의 차이점은 무엇인가?\n",
    "### 머신러닝\n",
    "- 어떤 특징을 추출하고 모델이 집어넣는다. 하지만 이러한 특징은 전문가의 지식이 필요함\n",
    "- 개념\n",
    "  - 데이터로 부터 명시적인 규칙 없이 패턴을 학습하는 알고리즘 전체를 의미\n",
    "  - SVM, DT, RF, LF 등 다양한 알고리즘이 존재\n",
    "- 특징 추출 방식\n",
    "    - 특징 엔지니어링(Feature Enginerring)이 중요하다.\n",
    "    - 도메인 지식을 활용하여 데이터를 전처리하고, 모델이 학습하기 좋은 형태의 특징을 직접 선택하거나 가공한다.\n",
    "  ```\n",
    "  특징 엔지니어링: 머신러닝 모델이 데이터를 효과적으로 학습할 수 있도록 원본 데이터에서 의미 있는 특징(Feature)을 추출, 변환 선택하는 과정\n",
    "  도메인 지식: 특정 분야나 문제 영역에 관한 전문적인 이해와 지식을 의미\n",
    "    - 예를 들어, 의료 분야는 환자의 증상, 질병의 원인, 치료방법을 의미하며, 자연어 처리에서는 언어의 문법, 어휘, 문화적 맥락이다.\n",
    "  ```\n",
    "- 데이터 요구량 및 계산 자원\n",
    "  - 상대적으로 적은양의 데이터로도 효과적으로 학습 가능\n",
    "  - 계산 자원도 딥러닝에 비해 적게 필요\n",
    "- 응용분야\n",
    "  - 구조가 비교적 단순한 문제에 많이 사용된다.\n",
    "  - 예로, 스팸 이메일 분류, 고객 세분화\n",
    "\n",
    "### 딥러닝\n",
    "- row Data를 모델에 특징을 추출하지 않고 모델 자체에서 특징을 추출해줌\n",
    "- 개념\n",
    "  - 머신러닝의 한 분야로, 인공 신경망(특히 다층 신경망)을 활용하여 복잡한 패턴을 자동으로 학습\n",
    "  - CNN, RNN, Transformer 등이 대표적\n",
    "- 특정 추출 방식\n",
    "  - 자동 특징 학습이 가능하다.\n",
    "  - 여러 계층(Layer)을 통해 입력 데이터에서 유용한 특징을 스스로 추출하므로, 복잡한 데이터(이미지, 음성, 자연어)에 효과적이다.\n",
    "- 데이터 요구량 및 계산 자원\n",
    "  - 좋은 성능을 내기 위해서는 대량의 데이터가 필요\n",
    "  - 여러 계층의 복잡한 네트워크 구조 떄문에 GPU 등 고성능 컴퓨팅 자우너이 요구됨\n",
    "- 응용분야\n",
    "  - 이미지 인식, 음성 인식, 자연어 처리 등 데이터의 복잡한 구조와 패턴을 다루는 문제에 주로 활용\n",
    "\n",
    "### 결론\n",
    "- 머신러닝과 딥러닝은 모두 데이터에서 패턴을 학습해 예측이나 분류 등의 작업을 수행하는 기술이다.\n",
    "- 머신러닝은 다양한 알고리즘을 포함하는 넓은 개념이며, 보통 특징 엔지니어링을 필요로 한다.\n",
    "- 딥러닝은 다층 신경망을 사용해 데이터를 자동으로 분석 및 특징을 추출하는 머신러닝의 한 하위 분야이다.\n"
   ],
   "id": "e72955196911bf5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Classification과 Regression의 주된 차이점은?\n",
    "### Classification(분류)\n",
    "- 출력 형태: 이산적 또는 범주형 결과가 나온다.\n",
    "- 목표: 입력 데이터를 미리 정의된 여러 클래스(레이블) 중 하나로 분류하는 것\n",
    "- 예시: 이메일 스팸 여부, 이미지의 객체 인식\n",
    "\n",
    "### Regression(회귀)\n",
    "- 출력 형태: 연속적 또는 실수 값\n",
    "- 목표: 입력 데이터로부터 숫자 값을 예측하는 것\n",
    "- 예시: 주택 가격 예측, 온도 예측, 주식 가격 예측\n",
    "- 회귀는 데이터를 통해 **연속적인 수치 값**을 예측하는 분석 방법이다.\n",
    "\n",
    "### 결론\n",
    "- 즉 이 둘의 차이는 출력의 형태이다.\n",
    "- 분류는 어떤 그룹에 속하는 가?를 묻는 문제이며\n",
    "- 회귀는 얼마나? 어떤수치가 나올까? 를 묻는 문제이다.\n"
   ],
   "id": "1f1836c68481fd0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. 머신러닝에서 차원의 저주(curse of dimensionality)란?\n",
    "### 개념\n",
    "- 차원이 증가하면서 학습 데이터 수가 차원 수보다 적어져서 성능이 저하되는 현상을 일컫는다.\n",
    "- 차원이 증가할수록 변수가 증가하고, 개별 차원 내에서 학습할 데이터 수가 적어진다.\n",
    "- 주의할 점은 변수가 많다고 반드시 차원의 저주가 발생하는 것은 아니며 관측치 보다 변수 수가 많아지는 경우 차원의 저주가 발생하는 것이다.\n",
    "- 즉, 데이터의 차원(특징 수)이 증가함에 따라 발생하는 여러 문제들을 총칭하는 개념이다.\n",
    "\n",
    "![차원의 저주](../images/readme/curseOfDimensionality.png)\n",
    "\n",
    "### 주요내용\n",
    "- 데이터의 희소성\n",
    "  - 차원이 높아지면 데이터가 고차원 공간에 퍼져서 각 데이터 포인트 간의 거리가 멀어진다.\n",
    "  - 결과적으로 데이터가 희소해진다.\n",
    "  - 이로 인해 패턴을 찾기 어려워지고, 유사성 측정의 무의미해질 수 있음\n",
    "- 모델 학습의 어려움\n",
    "  - 고차원 데이터에서는 학습에 필요한 데이터의 양이 기하급수적으로 늘어나야 한다.\n",
    "  - 즉, 충분한 데이터를 확보하지 못하면 모델이 과적합되기 쉽고, 일반화 성능이 떨어짐\n",
    "- 계산 복잡도 증가\n",
    "  - 특징의 수가 늘어날수록 연산량이 증가\n",
    "  - 학습 및 예측에 소요되는 시간과 메모리 비용이 크게 늘어난다.\n",
    "- 차원 축소 필요성\n",
    "  - 이러한 문제를 해결하기 위해 주성분 분석(PCA), t-SNE, UMAP 등의 차원 축소 기법을 사용\n",
    "  - 이처럼 데이터의 중요한 특징만 남기는 방법이 많이 사용된다."
   ],
   "id": "cc693fa739153465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Dimensionality Reduction(차원 축소)는 왜 필요한가?\n",
    "\n",
    "### 개념\n",
    "- 고차원 데이터를 보다 적은 차원으로 변환하는 과정이다.\n",
    "\n",
    "### 변환하는 이유\n",
    "- 차원의 저주 극복\n",
    "  - 위에서 설명한 것과 같이 고차원에서는 데이터가 희소해지며, 거리 측정이 어려움\n",
    "  - 이를 통해 학습 성능이 저하되는 문제를 방지 가능\n",
    "- 계산 비용 및 자원 절감\n",
    "  - 높은 차원은 모델 학습 시 더 많은 계산량과 메모리를 요구\n",
    "  - 차원 축소를 통해 데이터의 크기를 줄이면, 학습 속도를 높이고 자원 소모를 줄임\n",
    "- 노이즈 제거 및 중요한 정보 추출\n",
    "  - 모든 특성이 유용한 것은 아니다.\n",
    "  - 차원 축소는 데이터에서 불필요한 변수나 노이즈를 제거하고, 중요한 패턴을 담고 있는 특성만 남김으로 모델 성능 개선이 가능\n",
    "\n",
    "### 결론\n",
    "- 차원 축소는 데이터 전처리 과정에서 매우 중요한 역할을 하며, 복잡한 고차원 문제를 보다 효과적으로 해결할 수 있게 해줌\n"
   ],
   "id": "6d04c1874d3b90f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Ridge와 Lasso의 공통점과 차이점? (Regularization, 규제 , Scaling)\n",
    "### 라쏘와 릿지의 등장배경\n",
    "- 다중 선형 회귀모델은 과적합 되는 경향이 존재했음\n",
    "- 필요 이상으로 자세하게 특징값과 라벨값의 고계를 분석했다는 것이다.\n",
    "- 이로 인해 일반화 능력(범용 능력)이 떨어져 새로운 데이터를 제대로 예측하지 못했음\n",
    "- 이를 해결하기 위해 나온것이 라쏘와 릿지이다.\n",
    "### 라쏘\n",
    "- 기존의 선형 회귀에선 적절한 가중치와 편향을 찾아내는 것이 중요했음\n",
    "- 라쏘는 거기에 추가적인 제약 조건을 준다.\n",
    "- 바로 MSE가 초쇠가 되게하는 가중치와 편향을 찾으면서 동시에, 가중치들의 절대값의 합이 최소가 되게 한다.\n",
    "- 즉, 가중치의 모든 원소가 0이 되거나 0에 가깝게 되도록 해야 한다.\n",
    "- 따라서 어떠한 특징들을 모델을 만들 때 사용되지 않기도 함\n",
    "- 어떤 백터 요소의 절댓값의 합은 L1-norm이다.\n",
    "- 즉, 라쏘는 L1-norm 패널티를 가진 선형 회귀 방법임\n",
    "- 또한 MSE와 penalty항의 합이 최소가 되게 하는 w와b를 찾는것이 라쏘의 목적이다.\n",
    "### 릿지\n",
    "- 패널티 항에 L1-norm대신 L2-norm 패널티를 가진다.\n",
    "- 릿지의 가중치들은 0에 가까워질 뿐 0이 되지는 않는다.\n",
    "### 공통점\n",
    "1.정규화\n",
    "- 두 모델 모두 회귀 계수의 크기를 제한하여 과적합을 방지한다.\n",
    "- 손실 함수와 패널티를 추가하여 모델이 너무 복잡해지는것을 막는다.\n",
    "\n",
    "2. 스케일링이 중요\n",
    "- 릿지와 라쏘는 특성들의 크기에 영향을 받으므로, 표준화를 필수적으로 적용해야 한다.\n",
    "- 일반적으로 평균 0, 분산 1로 정규화한 후 사용하는 것이 효과적이다.\n",
    "\n",
    "### 차이점\n",
    "|      | 릿지 | 라쏘 |\n",
    "|------|---| ---------- |\n",
    "| 규제방식 | L2정규화 적용 | L1정규화 적용 |\n",
    "|특징선택| 모든 특성이 포함되지만, 계수의 크기를 줄임| 불필요한 특성의 계수를 0으로 만들어 자동으로 특성 선택 수행|\n",
    "|계수 감소 방식| 가중치(회귀 계수)가 0에 가깝게 축소되지만 완전히 0이 되지는 않음\\ 특정 가중치가 완전히 0이 되어 모델이 희소 해짐|\n",
    "|사용 예시| 다중 공선성 문제 해결에 효과적| 중요하지 안흥ㄴ 특성을 제거하여 해석 가능한 모델을 만들 때 유용|\n",
    "|성능 차이| 모든 특성을 사용하므로 일반적으로 예측 성능이 안정적 | 차원이 높은 데이터에서 변수선택이 필요할 때 더 효과적|\n",
    "\n",
    "\n",
    "### 결론\n",
    "- 특성이 많은데 그중 일부분만 중요하다면 라쏘\n",
    "- 특성의 중요도가 전체적으로 비슷하다면 릿지가 적합하다.\n",
    "### 참고\n",
    "[선형회귀, 라쏘, 릿지](https://otugi.tistory.com/127)\n"
   ],
   "id": "71f9b9509f5760c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Overfitting(과적합) vs. Underfitting(과소적합)\n",
    "|   | 과적합                        |과소적합|\n",
    "|----|----------------------------|-----|\n",
    "|정의| 훈련 데이터에 너무 잘맞아서 테스트 데이터에서 성능이 저하됨 | 훈련 데이터 조차 제대로 학습하지 못해 성능이 낮음|\n",
    "|원인| 모델이 너무 복잡함                 | 모델이 너무 단순함|\n",
    "|증상| 훈련데이터 성능 높고, 테스트 데이터 성능 낮음 | 훈련 데이터 성능 낮고, 테스트 데이터 성능 낮음|\n",
    "|해결책| 모델 단순화, 정규화(릿지,라쏘), Dropout 사용, 데이터 증가| 모델 복잡화, 학습 시간 증가, 더 많은 특성 추가|\n",
    "|예시| 다항 회귀에서 너무 높은 차수를 사용할 때| 선형 회귀로 비선형 데이터를 학습 할 때|\n"
   ],
   "id": "c66d7607a43105eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "10. Feature Engineering과 Feature Selection의 차이점은?\n",
    "### Feature Engineering(특성 공학)\n",
    "#### 정의\n",
    "- 새로운 특성을 생성하거나 변형하여 모델의 성능을 높이는 과정\n",
    "- 원본 데이터에서 유용한 정보를 추출하여 모델이 학습하기 쉬웅ㄴ 형태로 변환하는 것이 목적\n",
    "\n",
    "#### 주요 기법\n",
    "- 특성 변환(Transformation)\n",
    "  - 예: 로그변환, 제곱근 변환, 표준화\n",
    "- 특성 조합(Feature Combination)\n",
    "  - 여러 개의 특성을 조합하여 새로운 특성을 생성\n",
    "  - 예: 키 x 몸무게 -> BMI(체질량 지수)\n",
    "- 다항식 특성 추가\n",
    "  - 비선형 관계를 포착하기 위해 새로운 다항식 특성을 추가\n",
    "    - 예: x -> x^2, x^3\n",
    "- 텍스트 데이터 처리\n",
    "  - 단어 임베딩, TF-IDF 변환 등\n",
    "- 시간 관련 특성 추가\n",
    "  - \"날짜\" 데이터를 활용해 연도, 월, 요일, 계절 정보 추가\n",
    "\n",
    "#### 목표\n",
    "- 원본 데이터가 가진 정보를 최대한 활용하여 모델 성능을 극대화\n",
    "- 머신러닝 모델이 더 좋은 패턴을 학습할 수 있도록 새로운 특성을 추가\n",
    "\n",
    "\n",
    "### Feature Selection(특성 선택)\n",
    "#### 정의\n",
    "- 불필요한 특성을 제거하여 모델의 성능을 최적화 하는 과정\n",
    "- 모든 특성이 유용한 것은 아니므로, 모델 성능에 기여하지 않는 특성을 제거하여 차원을 줄이고 학습 속도를 높임\n",
    "\n",
    "#### 주요기법\n",
    "- 필터 방식\n",
    "  - 특성과 타겟 변수 간의 상관관계를 분석하여 중요한 특성만 선택\n",
    "  - 예: 피어슨 상관계수, 분산 임계값\n",
    "- 랩퍼 방식\n",
    "  - 모델을 사용하여 반복적으로 특성을 추가/제거하며 최적의 조합을 찾음\n",
    "  - 예: RFE\n",
    "- 임베디드 방식\n",
    "  - 모델이 학습하면서 자동으로 중요한 특성을 선택\n",
    "  - 예: 라쏘 회귀, 랜덤 포레스트의 feature Inportanace\n",
    "\n",
    "#### 목표\n",
    "- 모델이 불필요한 특성으로 인한 과적합을 방지\n",
    "- 모델의 계산 비용을 줄이고, 해석 가능성을 높임\n"
   ],
   "id": "acaed56ecb889709"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. 전처리(Preprocessing)의 목적과 방법? (노이즈, 이상치, 결측치)\n",
    "### 정의\n",
    "- 전처리는 모델이 데이터를 효과적으로 학습할 수 있도록 데이터를 정리하고 변환하는 과정\n",
    "\n",
    "### 목적\n",
    "#### 데이터 품질 향상\n",
    "- 데이터에 포함된 오류, 결측값, 이상치, 중복 데이터 등을 처리하여 신뢰성 있는 데이터를 제공\n",
    "- 정제된 데이터는 모델의 예측 성능을 향상시킴\n",
    "\n",
    "#### 모델 성능 최적화\n",
    "- 머신러닝 모델은 데이터의 스케일, 분포, 특성 등에 민감\n",
    "- 올바르게 전처리된 데이터는 모델이 학습하는데 적합하며, 과적합 또는 학습 오류를 줄일 수 있음\n",
    "\n",
    "#### 계산 효율성 증가\n",
    "- 데이터가 클수록 학습 시간이 증가하므로, 불필요한 데이터 제거 및 변환을 통해 속도를 개선\n",
    "- 예: 차원축소 등을 활용하여 모델의 연산량을 줄일 수 있음\n",
    "\n",
    "### 전처리 방법\n",
    "#### 노이즈 제거\n",
    "- 노이즈란 데이터 내에서 원래 정보가 아닌 불필요한 값, 오류, 무작위 변동성을 의미\n",
    "- 예: 잘못된 센서 데이터, OCR 인신 오류, 텍스트 데이터의 오타 등\n",
    "- 해결방법으로, 평균필터, 로우패스 필터, 텍스트 정제, 이미지 노이즈 제거가 있다.\n",
    "\n",
    "#### 이상치 처리\n",
    "- 이상치란 일반적인 데이터 분포에서 벗어난 비정상적인 값을 의미하낟.\n",
    "- 예: 신용카드 사기 거래 탐지, 센서 오작동 값, 데이터 입력 오류\n",
    "- 이상치 탐지방법에는 IQR, Z-Score- Isolation Forest, DBSCAN이 있다.\n",
    "\n",
    "#### 결측치 처리\n",
    "- 결측치란 데이터에서 값이 존재하지 않는 경우를 말한다.\n",
    "- 설문조사에서 응답 누락, 센서 데이터 손실, 잘못된 데이터 수집\n",
    "- 해결방법: 삭제, 평균/중앙값 대체, 최빈값 대체, KNN Imputation이 있다.\n",
    "\n",
    "### 결론\n",
    "- 전처리는 머신러닝에서 가장 중요한 과정 중 하나이며, 데이터를 정리하고 모델의 학습을 최적화 하는데 필수적이다.\n",
    "- 노이즈제거: 데이터에서 불필요한 정보를 제거\n",
    "- 이상치 처리: 극단적인 값이 모델 성능을 해치지 않도록 조정\n",
    "- 결측치 처리: 데이터 누락 문제를 해결하여 신뢰성 향상\n"
   ],
   "id": "aa15f8c34a41088b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "12. EDA(Explorary Data Analysis)란? 데이터의 특성 파악(분포, 상관관계)\n",
    "\n",
    "### 정의\n",
    "- 데이터 분선의 첫 단계로, 데이터의 구조와 특성을 파악하고, 이상치나 결측치를 발견하며, 데이터의 분포와 상관관계를 시각적으로 탐색하는 과정이다.\n",
    "- EDA를 수행하면 데이터의 패턴을 이해하고, 머신러닝 모델을 효과적으로 설계하기 위한 인사이트를 얻을 수 있다.\n",
    "\n",
    "### 목적\n",
    "- 데이터의 기본 구조 파악: 데이터의 크기, 타입, 기초 통계를 확인\n",
    "- 결측치 및 이상치 탐색: 데이터의 누락된 값이나 이상값을 찾음\n",
    "- 데이터 분포 분석: 변수의 분포를 확인하여 정규성 여부 등을 판단\n",
    "- 변수 간의 관계 분석: 상관관계, 다변수 관계 분석\n",
    "\n",
    "### EDA 과정\n",
    "1. 데이터 기본 정보확인\n",
    "- 먼저 데이터의 크기(행, 열 개수)와 변수(칼럼)의 타입을 확인한다.\n",
    "\n",
    "2. 기초 통계확인\n",
    "- 각 컬럼별로 데이터의 평균, 중앙값, 표준편차, 최솟값, 최댓값을 확인한다.\n",
    "\n",
    "3. 결측치 및 이상치 탐색\n",
    "- 결측치와 이상치를 탐색한 뒤 처리한다.\n",
    "\n",
    "4. 데이터 분포 시각화\n",
    "- 히스토그램을 통한 분포를 확인\n",
    "- 정규분포인지 확인하여 데이터 스케일링 방법 결정\n",
    "\n",
    "5. 변수 간의 관계 분석\n",
    "- 수치형 변수 간의 관계를 확인할 때 상관계수를 분석한다.\n",
    "\n",
    "### 결론\n",
    "- EDA를 통해 얻은 인사이트를 바탕으로 데이터 전처리 및 모델 설계 전략을 수릭할 수 있다.\n",
    "- EDA는 모델을 설계하기 전, 데이터를 이해하고 적절한 전처리 방안을 결정하는 중요한 과정이다.\n"
   ],
   "id": "e440fac70df8363b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "13. 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관되는가?\n",
    "### 개념\n",
    "- 회귀분석에서 절편, 기울기가 데이터를 설명하는 중요한 요소임\n",
    "- 이는 선형 회귀 뿐아니라, 딥러닝에서도 가중치와 편향 개념으로 확장된다.\n",
    "\n",
    "### 의미\n",
    "- 회귀 모델은 일반적으로 1차 함수인 선형 회귀로 표현됨\n",
    "- 즉, y = ax + b\n",
    "  - a: 기울기\n",
    "  - b: 절편\n",
    "  - x: 독립 변수\n",
    "  - y: 종속 변수\n",
    "- 즉, 기울기와 절편을 이용하여 입력값 x에 대해 y값을 예측하는 것이 회귀 분석의 핵심이다.\n",
    "- 여기서 기울기는 데이터의 변화율을 설명하고, 절편은 기준값을 설정하는 역할을 한다.\n",
    "\n",
    "### 딟러닝과 회귀 분석\n",
    "- 딥러닝에서 회귀분석의 기울기와 절편 개념은 가중치와 편향으로 확장된다.\n",
    "\n",
    "#### 가중치(weight)\n",
    "- 선형 회귀에서 기울기 a에 해당한다.\n",
    "- 뉴런이 입력 데이터를 처리하는 중요도를 나타내는 계수\n",
    "- 모델이 학습하면서 최적의 가중치를 찾아 데이터의 패턴을 학습\n",
    "- 예시: y = w[0]x[0] + w[1]x[1] + w[2]x[2] + b\n",
    "- w1,w12,w3는 각 입력 변수에 곱해지는 가중치\n",
    "- b는 편향 즉, 절편을 의미\n",
    "- 즉, 뉴런은 여러 개의 입력을 받고, 각 입력에 가중치를 곱한 후 합산하여 출력값을 결정\n",
    "\n",
    "#### 편향(Bias)\n",
    "- 절편에 해당\n",
    "- 뉴런이 입력이 0이어도 일정한 출력을 내게 하는 역할\n",
    "- 딥러닝 모델이 특정 패턴을 학습할 수 있도록 유연성을 제공\n",
    "- 예: y= w[0]x[0] +b\n",
    "- x[0] = 0이면, y = b\n",
    "- 즉, 편향은 모델이 입력이 없어도 학습된 기본값을 유지하도록 도와줌\n",
    "\n",
    "#### 선형 회귀와의 유사성\n",
    "|개념| 선형회귀 | 딥러닝(뉴런)|\n",
    "|-|-|-|\n",
    "|기울기| 독립 변수 x의 변화량| 가중치(W)|\n",
    "|절편| x = 0일때의 기본값 | 편향(Bias,b)|\n",
    "|예측값| y=ax+b| y=Wx+b|\n",
    "|학습과정| 최소제곱법으로 최적의 기울기, 절편 찾음| 경사하강법으로 최적의 가중치, 편향 찾음|\n",
    "\n",
    "#### 경사 하강법 학습 과정\n",
    "1. 초기 가중치와 편향을 랜덤하게 설정\n",
    "2. 손실 함수를 계산\n",
    "3. 기울기를 계산하여 가중치 업데이트\n",
    "   -편미분을 이용해 각 가중치 W와 편향 b에 대한 변화량 계산\n",
    "4. 반복적으로 업데이트하여 최적의 값 찾기\n",
    "\n",
    "#### 결론\n",
    "- 회귀 분석의 기울기와 절편은 딥러닝에서 가중치와 편향으로 확장\n",
    "- 기울기는 입력 변수의 변화량을 설명\n",
    "- 절편은 기본값을 설정\n",
    "- 딥러닝에서는 경사 하강법을 통해 최적의 가중치와 편향을 학습하여 데이터의 패턴을 학습함\n",
    "- 선형 회귀는 딥러닝의 기본 개념으로, 신경망에서도 가중치와 편향을 학습하는 방식이 유사함\n",
    "- 즉, 회귀 모델은 딥러닝의 가장 기초적인 형태이며, 신경망이 더 복잡해지면 다층 퍼셉트론(MLP), CNN, RNN 등으로 확장될 뿐, 기본 원리는 동일\n"
   ],
   "id": "422963e2c7716b70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##14. GRU을 사용하는 이유와 차별성은?\n",
    "### 정의\n",
    "- GRU(gated Recurrent Unit)는 RNN의 한 종류이며, LSTM과 유사하지만 더 간단한 구조를 갖는다.\n",
    "- RNN의 문제점을 개선하면서 시계열 데이터, 자연어 처리(NLP), 음성인식 등 다양한 분야에서 사용됨\n",
    "\n",
    "### 사용 이유\n",
    "#### 기존 RNN의 한계\n",
    "- 장기 의존성 문제\n",
    "  - 일반적인 RNN은 이전 정보를 현재 상태로 전달하지만, 오래된 정보를 잊어버리는 경향이 있음\n",
    "  - 즉, 긴 문장이나 긴 시계열 데이터에서는 초반 정보가 손실된다.\n",
    "- 기울기 소실 문제\n",
    "  - RNN은 역전파 과정에서 기울기가 점점 작아지는 문제가 발생\n",
    "  - 이로 인해 초반 데이터가 학습되지 않고, 가중치 업데이트가 잘 되지 않음\n",
    "\n",
    "#### GRU의 장점\n",
    "- LSTM(Long Short-Term Memory)처럼 게이트 구조를 사용하여 정보 흐름을 조절하여 장기 의존성을 해결\n",
    "- LSTM보다 구조가 단순하여 계산 속도가 빠름\n",
    "- 적은 데이터로도 효과적인 성능을 낼 수 있음\n",
    "\n",
    "### GRU의 차별점\n",
    "- GRU는 LSTM의 Cell State를 제거하고, Hidden State 하나로만 정보 전달\n",
    "- GRU는 Reset Gate와 Update Gate만 사용하여 LSTM보다 구조가 단순함\n",
    "- GRU는 연산량이 적어 빠르지만, LSTM은 더 정밀한 패턴 학습이 가능함\n"
   ],
   "id": "ffd2d1761237e54e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 15. 결정트리에서 불순도(Impurity) – 지니 계수(Gini Index)란 무엇인가?\n",
    "### 불순도\n",
    "#### 정의\n",
    "- 불순도는 결정 트리에서 특정 노드의 데이터가 얼마나 혼합 되어있는지를 나타내는 지표이다.\n",
    "- 즉, 한 노드에 다양한 클래스가 섞여 있을수록 불순도가 높으며, 한 가지 클래스만 포함되면 불순도가 낮음\n",
    "- 결정트리는 데이터를 가장 잘 구분하는 기준을 찾아 트리를 분할한다.\n",
    "- 트리가 분할될 떄, 불순도가 낮아지는 방향으로 최적의 기준을 찾음\n",
    "- 불순도가 낮아질수록 예측이 정확해지고, 모델의 성능이 향상됨\n",
    "\n",
    "#### 측정방법\n",
    "- 불순도는 지니계수와 엔트로피 방식으로 측정이 가능하다.\n",
    "- 지니계수: 각 클래스로 데이터가 분포될 확률을 사용하여 불순도를 측정\n",
    "- 엔트로피: 정보이론 기반의 개념을 활용하여 불순도를 측정\n",
    "\n",
    "### 지니 계수\n",
    "#### 정의\n",
    "- 지니 계수는 결정트리에서 특정 노드가 얼마나 순수(Pure)한지 측정하는 지표\n",
    "- 지니 계수는 클래스가 섞여 있을 확률을 기반으로 계산된다.\n",
    "- 즉, 모든 클래스의 확률을 제곱한 값을 합한한 후, 1에서 뺴는 방식으로 계산 된다.\n",
    "- 불순도가 높을수록 지니계수가 커지고, 불순도가 낮을수록 지니 계수가 작아진다.\n",
    "\n",
    "### 결론\n",
    "- 지니계수는 결정트리에서 불순도를 측정하는 중요한 지표\n",
    "- 클래스가 얼마나 혼합되어 있는지 수치적으로 나타내며, 값이 낮을 수록 데이터가 순수\n",
    "- 결정트리는 지니계수가 가장 작은 방향으로 데이터를 분할하여 예측 성능을 향상\n",
    "- 지니 계수는 엔트로피보다 계산량이 적어 빠르고, 대부분의 결정트리 모델에서 기본값을 사용됨\n",
    "- 즉, 지니계수는 '가장 순수한 분할을 찾기 위한 기준'이다."
   ],
   "id": "ff23fdd33d8ea699"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 16. 앙상블이란 무엇인가?\n",
    "### 정의\n",
    "- 앙상블이란 여러 개의 머신러닝 모델을 조합하여 최종적인 예측 성능을 향상시키는 기법이다.\n",
    "- 즉, 개별 모델의 단점을 보완하고 강점을 극대화하여 더 정확하고 안정적인 결과를 얻는 방법이다.\n",
    "\n",
    "### 사용이유\n",
    "#### 단일 모델의 한계\n",
    "- 하나의 모델이 데이터를 학습하면, 특정한 패턴에 과적합 될 가능성이 높음\n",
    "- 단일 모델은 특정 유형의 데이터에는 강하지만, 다른 유형에서는 약할 수 있음\n",
    "- 즉, 여러 개의 모델을 결합하여 개별 모델보다 더 강력한 성능을 보일 수 있다.\n",
    "\n",
    "#### 장점\n",
    "- 정확도 향상: 여러 모델의 결과를 종합하면 일반적으로 성능이 향상됨\n",
    "- 과적합 방지: 개별 모델이 과적합되더라도, 여러 모델을 조합하면 일반화 성능이 높아짐\n",
    "- 안정성 증가: 한 개의 모델이 잘못된 예측을 하더라도 다른 모델들이 이를 보완\n"
   ],
   "id": "3c263b2e6b4ca961"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 17. 부트 스트랩핑(bootstraping)이란 무엇인가?\n",
    "### 정의\n",
    "- 주어진 데이터에서 여러 개의 샘플을 무작위로 추출하여 새로운 데이터셋을 만드는 통계적 기법\n",
    "- 이를 사용하면 데이터가 부족한 상황에서도 다양한 샘플을 만들어 신뢰도 높은 모델을 만들 수 있음\n",
    "\n",
    "### 방식\n",
    "1. 원본 데이터셋에서 중복을 허용하여 랜덤하게 샘플을 추출한다.\n",
    "2. 동일한 크기의 새로운 데이터셋을 여러개 생성\n",
    "3. 각 샘플을 독립적으로 학습하여 결과를 종합\n",
    "4. 예시\n",
    "   ```\n",
    "       원본 데이터: [A, B, C, D, E]\n",
    "       부트스트랩 샘플 1: [B, C, C, E, A]\n",
    "       부트스트랩 샘플 2: [E, D, D, A, C]\n",
    "       부트스트랩 샘플 3: [A, A, B, D, E]\n",
    "    ```\n",
    "### 사용 이유\n",
    "#### 데이터가 부족할 떄 효과적인 학습 방법\n",
    "- 기존 데이터에서 랜덤 샘플링을 통해 여러 개의 학습용 데이터 셋을 생성\n",
    "- 작은 데이터셋에서도 통계적으로 신뢰할 수 있는 결과 도출 가능\n",
    "\n",
    "#### 추정치의 신뢰도 향상\n",
    "- 단일 샘플이 아닌 여러 개의 부트스트랩 샘플을 활용하여 평균, 분산 등을 계산\n",
    "- 모델의 안정성을 높이고, 과적합을 방지할 수 있음\n",
    "\n",
    "#### 앙상블 학습의 핵심\n",
    "- 배깅에서 부트스트래핑을 사용하여 랜덤한 데이터셋을 생성\n"
   ],
   "id": "7202b97cdfa26ec6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 18. 배깅(Bagging)이란 무엇인가?\n",
    "### 정의\n",
    "- 여러 개의 약한 학습기(모델)를 독립적으로 학습한 후, 예측 결과를 평균(회귀) 도는 투표(분류)하여\n",
    "- 최종 결과를 결정하는 앙상블 기법\n",
    "- 배깅은 부트스트래핑을 활용하여 데이터 샘플을 랜덤하게 생성하고, 여러 개의 모델을 학습하여 예측을 안정화 하는 것이다.\n",
    "\n",
    "### 배깅 과정\n",
    "1. 부트스트램 샘플링\n",
    "- 원본 데이터에서 중복을 허용하여 여러 개의 새로운 데이터셋을 생성\n",
    "- 예를 들어, 원본 데이터가 100개라면, 100개를 랜덤하게 뽑되 중복이 가능\n",
    "\n",
    "2. 독립적인 모델 학습\n",
    "- 여러 개의 모델을 각각 다른 부트스트랩 샘플에 대해 학습\n",
    "\n",
    "3. 예측값을 결합하여 최종 예측 수행\n",
    "- 분류: 다수결 투표\n",
    "- 회귀: 평균\n",
    "\n",
    "### 사용이유\n",
    "#### 단일 모델의 한계를 극복\n",
    "- 하나의 모델이 과적합될 가능성이 높음\n",
    "- 여러개의 모델을 조합하면 과적합을 방지\n",
    "- 데이터의 작은 변화에 따라 결과가 크게 달라지는 문제 해결 가능\n",
    "\n",
    "#### 과적합 방지\n",
    "- 랜덤한 부트스트랩 샘플을 사용하여 모델을 독립적으로 학습하면, 개별 모델이 특정 패턴에 과적합 되는 것을 방지\n",
    "\n",
    "#### 일반화 성능 향상\n",
    "- 배깅을 사용하면 편향은 그대로 유지되지만, 분산이 줄ㄹ어들어 더 일반화된 모델을 생성\n",
    "\n",
    "### 결론\n",
    "- 배깅은 데이터의 랜덤성을 활용하여 안정적인 예측을 수행하는 강력한 방법이다.\n"
   ],
   "id": "6a14f9014433a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 19. 주성분 분석(PCA) 이란 무엇인가?\n",
    "### 정의\n",
    "- 고차원의 데이터를 저차원으로 변환하여 핵심 정보를 유지하면서 **차원을 축소**하는 방법이다.\n",
    "- 즉, 데이터의 중요한 패턴을 유지하면서 불필요한 특성을 제거하여 더 효율적으로 분석할 수 있도록 변환하는 기법\n",
    "\n",
    "### 필요한 이유\n",
    "1. 고차원 데이터의 시각화\n",
    "- 차원이 높은 데이터를 2D 또는 3D로 변환하여 시각화 가능\n",
    "\n",
    "2. 차원의 저주 해결\n",
    "- 차원이 높아질수록 모델의 성능이 저하될 수 있음\n",
    "\n",
    "3. 연산 속도 향상\n",
    "- 머신러닝 모델 학습 시, 차원이 낮으면 연산량이 줄어 속도가 빨라진다.\n",
    "\n",
    "4. 중복 정보 제거\n",
    "- 서로 상관관계가 높은 특성을 줄여 데이터의 핵심 정보를 유지\n",
    "\n",
    "### 작동 원리\n",
    "1. 데이터를 평균이 0이 되도록 정규화\n",
    "2. 공분산 행렬 계산\n",
    "3. 공분산 행렬의 고유값과 고유벡터 계산\n",
    "4. 가장 큰 고유값을 가진 고유벡터를 주성분으로 선택\n",
    "5. 데이터를 주성분 방향으로 변환하여 새로운 저차원 데이터 생성\n",
    "즉, 데이터를 가장 잘 설명하는 새로운 축을 찾고 중요한 축을 몇개만 선택하여 차원을 축소하는 방법\n",
    "\n"
   ],
   "id": "e23df79e9f412d34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a600e571d64db155"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
